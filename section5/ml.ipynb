{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Machine Learning\n",
    "Using the dataset from https://archive.ics.uci.edu/ml/datasets/Car+Evaluation, create a machine learning model to predict the buying price given the following parameters:\n",
    "\n",
    "Maintenance = High\n",
    "Number of doors = 4\n",
    "Lug Boot Size = Big\n",
    "Safety = High\n",
    "Class Value = Good\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Price  Maint doors persons lug_boot safety  class\n",
      "0  vhigh  vhigh     2       2    small    low  unacc\n",
      "1  vhigh  vhigh     2       2    small    med  unacc\n",
      "2  vhigh  vhigh     2       2    small   high  unacc\n",
      "3  vhigh  vhigh     2       2      med    low  unacc\n",
      "4  vhigh  vhigh     2       2      med    med  unacc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count     1728\n",
       "unique       4\n",
       "top          2\n",
       "freq       432\n",
       "Name: doors, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Data Exploration\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "mydf = pd.read_csv('cardata.csv')\n",
    "\n",
    "print(mydf.head(5))\n",
    "mydf['doors'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is completely categorical.\n",
    "\n",
    "Use onehotencoding to perform simple logistic regression and see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "target = mydf['class']\n",
    "X = mydf.loc[:, ~mydf.columns.isin(['class','Price'])]\n",
    "\n",
    "enc.fit(X)\n",
    "\n",
    "enc.categories_\n",
    "independ_variables_OHE = enc.transform(X).toarray()\n",
    "# enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]])\n",
    "# enc.get_feature_names_out(['gender', 'group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    independ_variables_OHE, \n",
    "    target, \n",
    "    test_size=0.33, \n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8266199649737302\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create an instance of Logistic Regression Classifier and fit the data.\n",
    "logreg = LogisticRegression(C=1e5)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "print(logreg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations from first round of data.\n",
    "\n",
    "Very poor accuracy from simple logistic regression. Some exploration of cooccurrence has to be done., and feature extraction can be done. Try Decision trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7618213660245184\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worse. Do some data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vhigh    432\n",
      "high     432\n",
      "med      432\n",
      "low      432\n",
      "Name: Price, dtype: int64\n",
      "vhigh    432\n",
      "high     432\n",
      "med      432\n",
      "low      432\n",
      "Name: Maint, dtype: int64\n",
      "2        432\n",
      "3        432\n",
      "4        432\n",
      "5more    432\n",
      "Name: doors, dtype: int64\n",
      "2       576\n",
      "4       576\n",
      "more    576\n",
      "Name: persons, dtype: int64\n",
      "small    576\n",
      "med      576\n",
      "big      576\n",
      "Name: lug_boot, dtype: int64\n",
      "low     576\n",
      "med     576\n",
      "high    576\n",
      "Name: safety, dtype: int64\n",
      "unacc    1210\n",
      "acc       384\n",
      "good       69\n",
      "vgood      65\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for col in mydf.columns:\n",
    "    print(mydf[col].value_counts())   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row is unique set of data. Dataset is not suitable for predicting Price using the other independent variables. Try to run regression purely on the supposed dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['acc', 'good', 'unacc', 'vgood'], dtype=object)]\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "0.318739054290718\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "target = mydf['Price']\n",
    "X = mydf.loc[:, ['class']]\n",
    "\n",
    "enc.fit(X)\n",
    "\n",
    "enc.categories_\n",
    "independ_variables_OHE = enc.transform(X).toarray()\n",
    "# enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]])\n",
    "# enc.get_feature_names_out(['gender', 'group'])\n",
    "print(enc.categories_)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    independ_variables_OHE, \n",
    "    target, \n",
    "    test_size=0.33, \n",
    "    random_state=42)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create an instance of Logistic Regression Classifier and fit the data.\n",
    "logreg = LogisticRegression(C=1e5)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "print(logreg.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17fcbb0f56c05b0e1bde87ffb8d487166277a81d1edd56a2172126f6ef6c36cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
